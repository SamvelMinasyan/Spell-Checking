# Spell Checker Evaluation Documentation

## Overview
This repository contains scripts to evaluate the performance of multiple spell checkers (TextBlob, SymSpell) using a dataset with synthetic spelling errors.

## Data Preparation
- **Dataset**: We use a public English corpus with synthetic errors introduced for testing.
- **Synthetic Errors**: Typos are generated by swapping letters, randomly deleting characters, or adding extra letters.

## Spell Checkers
1. **TextBlob**: A simple, dictionary-based correction tool.
2. **SymSpell**: Efficient for large text with common misspellings.
3. Could use different LLMs thorugh APIs too, just need to add the function in evaluation.py in the same way as for 
   other ones.

## Evaluation Metrics
- **Accuracy**: Percentage of words correctly identified and corrected.
- **Precision, Recall, F1 Score**: Calculated on a word-by-word basis for true positive, false positive, and false negative rates.
- **Edit Distance**: Measures similarity between corrected and original text.

## Running the Evaluation
1. Prepare the data:
   ```bash
   python data/data_preparation.py
